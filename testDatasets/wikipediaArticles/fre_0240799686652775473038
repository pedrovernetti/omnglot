
## Variables indépendantes et identiquement distribuées

En théorie des probabilités et en statistique, des variables indépendantes et identiquement distribuées sont des variables aléatoires qui suivent toutes la même loi de probabilité et sont indépendantes. On dit que ce sont des variables aléatoires iid ou plus simplement des variables iid.
Un exemple classique de variables iid apparait lors d'un jeu de pile ou face, c'est-à-dire des lancers successifs d'une même pièce. Les variables aléatoires qui représentent chaque résultat des lancers (0 pour face et 1 pour pile) suivent toutes la même loi de Bernoulli. De plus les lancers étant successifs, les résultats n'ont pas de lien de dépendance entre eux et ainsi les variables aléatoires sont indépendantes.
L'apparition de variables iid se retrouve régulièrement en statistique. En effet, lorsque l'on étudie un caractère sur une population, on réalise un échantillon : on sélectionne une partie de la population, on mesure le caractère étudié et on obtient ainsi une série de valeurs qui sont supposées aléatoires, indépendantes les unes des autres et qui sont modélisées par des variables aléatoires avec une loi de probabilité adaptée. Inversement, lorsque l'on récupère des données statistiques, des méthodes permettent de savoir si elles sont issues de variables iid. 
Plusieurs théorèmes de probabilité nécessitent l'hypothèse de variables iid. En particulier le théorème central limite dans sa forme classique énonce que la somme renormalisée de variables iid tend vers une loi normale. C'est également le cas de la loi des grands nombres qui assure que la moyenne de variables iid converge vers l'espérance de la loi de probabilité des variables. 
Des méthodes de calcul comme la méthode de Monte-Carlo utilisent des variables iid. Il est alors utile de savoir simuler informatiquement celles-ci ; ces valeurs simulées sont dites pseudo-aléatoires car obtenir des valeurs parfaitement iid est impossible. Les algorithmes utilisant la congruence sur les entiers ne donnent pas d'indépendance parfaite, on parle dans ce cas de hasard faible. En utilisant des phénomènes physiques, il est possible d'obtenir de meilleures valeurs pseudo-aléatoires, il s'agit de hasard fort.

# Origines et explications 

Dans l'histoire des probabilités, les premiers raisonnements tels que le problème des partis au XVIIe siècle, ont été faits sur des jeux de hasard. Il est question d'un jeu en plusieurs parties ; même si le terme de variables indépendantes et identiquement distribuées n'est pas utilisé, il en est question : « le hasard est égal ». Dans le problème du jeu de dés posé par le chevalier de Méré, il admet que « les faces du dé sont également possibles et par là-même qu'elles ont la même chance de se produire ». 
C'est toujours au XVIIe siècle que Jérome Cardan, Pierre de Fermat et Christian Huygens utilisent la notion d'équiprobabilité, c'est-à-dire que certains événements ont la même probabilité d'apparaître, autrement dit que certaines variables suivent la même loi. La notion d'indépendance n'apparaît que plus tard avec Abraham de Moivre.
Dans les mêmes années, des questions se posent en démographie au sujet des rentes viagères. Ces questions statistiques s'appuient sur les travaux probabilistes naissants des jeux de hasard pour évaluer des valeurs des rentes. Leibniz considère ces questions comme analogues aux jeux de hasard, c'est-à-dire dans le cas d’événements indépendants et de même chance : les fréquences des décès sont considérées comme des valeurs de probabilité, la durée comme le gain dans un jeu contre la nature et les calculs sur plusieurs personnes se font par multiplication.
Ainsi les exemples du lancer du dé, du jeu de pile ou face ou du tirage aléatoire avec remise de boules dans une urne sont des exemples classiques que l'on modélise par des variables indépendantes et identiquement distribuées. Pour ces trois exemples, chaque tirage ou lancer se fait dans les mêmes situations (même dé, même pièce ou même contenu de l'urne grâce au tirage avec remise), ainsi les résultats de chaque tirage suivent la même loi de probabilité. De plus les tirages ou lancers ne dépendent pas des résultats des tirages et lancers précédents (encore dû à la remise des boules), les variables aléatoires sont donc indépendantes.
En statistique, un échantillon est construit en « tirant au hasard » des individus dans une population. Le terme « au hasard » sous-entend l'hypothèse d'équiprobabilité, c'est-à-dire qu'à chaque tirage, les individus ont la même chance d'être prélevés, ce qui assure que les résultats du tirage sont de même loi. Pour avoir l'indépendance, le tirage peut être réalisé avec remise. Dans le cas où la population totale est très grande ou considérée infinie, le fait de tirer avec ou sans remise ne modifie pas l'indépendance des résultats.
De tels résultats obtenus par plusieurs observations successives d'un même phénomène aléatoire sont appelés des échantillons aléatoires.

# Définition 

La désignation « indépendantes et identiquement distribuées » regroupe deux notions : l'indépendance et la loi de probabilité. L'indépendance est une propriété qui régit les relations des variables entre elles, tandis que la notion d'identiquement distribuée se réfère à la faculté de chacune des variables de suivre la même loi de probabilité que les autres variables de la suite. Des variables indépendantes et identiquement distribuées sont dites iid, i.i.d. ou indépendantes et équidistribuées.
Intuitivement, dans le cas de variable aléatoire réelle, l'indépendance entre deux variables aléatoires signifie que la connaissance de l'une n'influe en rien sur la valeur de l'autre. 
Donnons les cas particuliers de deux variables aléatoires discrètes ne pouvant prendre qu'un nombre fini de valeurs, et de deux variables aléatoires continues, c'est-à-dire possédant une densité de probabilité.

Une manière plus générale est de considérer les fonctions de répartition : les variables 
 
 
 
 X
 ,
 Y
 
 
 {\displaystyle X,Y}
 ont même loi si elles ont la même fonction de répartition et sont dites indépendantes si 
 
 
 
 
 F
 
 X
 ,
 Y
 
 
 (
 x
 ,
 y
 )
 

 
 F
 
 X
 
 
 (
 x
 )
 ×
 
 F
 
 Y
 
 
 (
 y
 )
 
 
 {\displaystyle F_{X,Y}(x,y)=F_{X}(x)\times F_{Y}(y)}
 pour tous réels 
 
 
 
 x
 ,
 y
 
 
 {\displaystyle x,y}
 où 
 
 
 
 
 F
 
 X
 ,
 Y
 
 
 ,
 
 F
 
 X
 
 
 
 
 {\displaystyle F_{X,Y},F_{X}}
 et 
 
 
 
 
 F
 
 Y
 
 
 
 
 {\displaystyle F_{Y}}
 sont les fonctions de répartition respectives du couple 
 
 
 
 (
 X
 ,
 Y
 )
 
 
 {\displaystyle (X,Y)}
 et des variables 
 
 
 
 X
 
 
 {\displaystyle X}
 et 
 
 
 
 Y
 
 
 {\displaystyle Y}
 . Cette définition se généralise pour 
 
 
 
 n
 
 
 {\displaystyle n}
 variables aléatoires indépendantes, dites mutuellement indépendantes.
Plus mathématiquement, des variables iid sont supposées être définies sur le même espace, c'est-à-dire qu'il existe un espace probabilisé 
 
 
 
 (
 Ω
 ,
 
 
 F
 
 
 ,
 
 P
 
 )
 
 
 {\displaystyle (\Omega ,{\mathcal {F}},\mathbb {P} )}
 tel que chaque variable aléatoire 
 
 
 
 
 X
 
 i
 
 
 
 
 {\displaystyle X_{i}}
 est une fonction mesurable 
 
 
 
 
 X
 
 i
 
 
 :
 Ω
 →
 
 R
 
 
 
 {\displaystyle X_{i}:\Omega \rightarrow \mathbb {R} }
 .

De manière plus générale, il est possible de considérer des variables aléatoires à valeurs non réelles, c'est-à-dire dans un espace mesurable général. Dans ce cas, chaque variable aléatoire est une fonction mesurable d'un espace probabilisé 
 
 
 
 (
 Ω
 ,
 
 
 F
 
 
 ,
 
 P
 
 )
 
 
 {\displaystyle (\Omega ,{\mathcal {F}},\mathbb {P} )}
 dans un espace mesurable. 
 
 
 
 X
 :
 (
 Ω
 ,
 
 
 F
 
 
 ,
 
 P
 
 )
 ⟶
 (
 
 E
 
 X
 
 
 ,
 
 
 
 E
 
 
 
 X
 
 
 )
 
 
 {\displaystyle X:(\Omega ,{\mathcal {F}},\mathbb {P} )\longrightarrow (E_{X},{\mathcal {E}}_{X})}
 et 
 
 
 
 Y
 :
 (
 Ω
 ,
 
 
 F
 
 
 ,
 
 P
 
 )
 ⟶
 (
 
 E
 
 Y
 
 
 ,
 
 
 
 E
 
 
 
 Y
 
 
 )
 
 
 {\displaystyle Y:(\Omega ,{\mathcal {F}},\mathbb {P} )\longrightarrow (E_{Y},{\mathcal {E}}_{Y})}
 .
Ainsi, on dit que les variables 
 
 
 
 X
 
 
 {\displaystyle X}
 et 
 
 
 
 Y
 
 
 {\displaystyle Y}
 sont indépendantes et identiquement distribuées si :

les événements 
 
 
 
 
 X
 
 −
 1
 
 
 (
 A
 )
 
 
 {\displaystyle X^{-1}(A)}
 et 
 
 
 
 
 Y
 
 −
 1
 
 
 (
 B
 )
 
 
 {\displaystyle Y^{-1}(B)}
 sont indépendants pour tous 
 
 
 
 A
 ∈
 
 
 
 E
 
 
 
 X
 
 
 
 
 {\displaystyle A\in {\mathcal {E}}_{X}}
 et 
 
 
 
 B
 ∈
 
 
 
 E
 
 
 
 Y
 
 
 
 
 {\displaystyle B\in {\mathcal {E}}_{Y}}
 ,
les mesures images 
 
 
 
 
 
 P
 
 
 X
 
 
 
 
 {\displaystyle \mathbb {P} _{X}}
 et 
 
 
 
 
 
 P
 
 
 Y
 
 
 
 
 {\displaystyle \mathbb {P} _{Y}}
 de 
 
 
 
 
 P
 
 
 
 {\displaystyle \mathbb {P} }
 par 
 
 
 
 X
 
 
 {\displaystyle X}
 et par 
 
 
 
 Y
 
 
 {\displaystyle Y}
 sont égales.Ces définitions se généralisent au cas d'une famille quelconque finie, dénombrable ou infinie non-dénombrable de variables aléatoires.

# Propriétés 

Les variables iid apparaissent dans beaucoup de situations et résultats statistiques et probabilistes car elles possèdent de nombreuses propriétés qui permettent de mieux étudier leur loi de probabilité ou leur somme notamment. Dans la suite de cette section, 
 
 
 
 
 X
 
 1
 
 
 ,
 
 X
 
 2
 
 
 ,
 …
 ,
 
 X
 
 n
 
 
 
 
 {\displaystyle X_{1},X_{2},\dots ,X_{n}}
 sont des variables réelles indépendantes et identiquement distribuées.
Si on note 
 
 
 
 
 f
 
 X
 
 
 
 
 {\displaystyle f_{X}}
 la densité de probabilité du vecteur 
 
 
 
 X
 

 (
 
 X
 
 1
 
 
 ,
 
 X
 
 2
 
 
 ,
 …
 ,
 
 X
 
 n
 
 
 )
 
 
 {\displaystyle X=(X_{1},X_{2},\dots ,X_{n})}
 et 
 
 
 
 f
 
 
 {\displaystyle f}
 la densité commune des variables 
 
 
 
 
 X
 
 1
 
 
 ,
 
 X
 
 2
 
 
 ,
 …
 ,
 
 X
 
 n
 
 
 
 
 {\displaystyle X_{1},X_{2},\dots ,X_{n}}
 , alors : 
 
 
 
 
 f
 
 X
 
 
 (
 
 t
 
 1
 
 
 ,
 
 t
 
 2
 
 
 ,
 …
 ,
 
 t
 
 n
 
 
 )
 

 f
 (
 
 t
 
 1
 
 
 )
 f
 (
 
 t
 
 2
 
 
 )
 …
 f
 (
 
 t
 
 n
 
 
 )
 
 
 {\displaystyle f_{X}(t_{1},t_{2},\dots ,t_{n})=f(t_{1})f(t_{2})\dots f(t_{n})}
 . Autrement dit, la fonction de densité d'un 
 
 
 
 n
 
 
 {\displaystyle n}
 -uplet de variables iid est à variables séparables.
La somme de variables iid possède de bonnes propriétés ; donnons ici les principales. Si l'espérance commune des variables 
 
 
 
 
 X
 
 i
 
 
 
 
 {\displaystyle X_{i}}
 est 
 
 
 
 μ
 
 
 {\displaystyle \mu }
 et la variance commune est 
 
 
 
 
 σ
 
 2
 
 
 
 
 {\displaystyle \sigma ^{2}}
 , alors la somme et la moyenne ont pour espérance et pour variance :

 
 
 
 
 E
 
 (
 
 X
 
 1
 
 
 +
 
 X
 
 2
 
 
 +
 ⋯
 +
 
 X
 
 n
 
 
 )
 

 n
 μ
 
 
 {\displaystyle \mathbb {E} (X_{1}+X_{2}+\dots +X_{n})=n\mu }
 et 
 
 
 
 Var
 
 (
 
 X
 
 1
 
 
 +
 
 X
 
 2
 
 
 +
 ⋯
 +
 
 X
 
 n
 
 
 )
 

 n
 
 σ
 
 2
 
 
 
 
 {\displaystyle \operatorname {Var} (X_{1}+X_{2}+\dots +X_{n})=n\sigma ^{2}}
 ,

 
 
 
 
 E
 
 
 (
 
 
 
 
 X
 
 1
 
 
 +
 
 X
 
 2
 
 
 +
 ⋯
 +
 
 X
 
 n
 
 
 
 n
 
 
 )
 
 

 μ
 
 
 {\displaystyle \mathbb {E} \left({\frac {X_{1}+X_{2}+\dots +X_{n}}{n}}\right)=\mu }
 et 
 
 
 
 Var
 
 
 (
 
 
 
 
 X
 
 1
 
 
 +
 
 X
 
 2
 
 
 +
 ⋯
 +
 
 X
 
 n
 
 
 
 n
 
 
 )
 
 

 
 
 
 σ
 
 2
 
 
 n
 
 
 
 
 {\displaystyle \operatorname {Var} \left({\frac {X_{1}+X_{2}+\dots +X_{n}}{n}}\right)={\frac {\sigma ^{2}}{n}}}
 .Si on note 
 
 
 
 Ψ
 
 
 {\displaystyle \Psi }
 la fonction caractéristique commune des variables 
 
 
 
 
 X
 
 i
 
 
 
 
 {\displaystyle X_{i}}
 et 
 
 
 
 
 Ψ
 
 
 S
 
 n
 
 
 
 
 
 
 {\displaystyle \Psi _{S_{n}}}
 celle de la somme 
 
 
 
 
 S
 
 n
 
 
 

 
 X
 
 1
 
 
 +
 
 X
 
 2
 
 
 +
 ⋯
 +
 
 X
 
 n
 
 
 
 
 {\displaystyle S_{n}=X_{1}+X_{2}+\dots +X_{n}}
 , alors

 
 
 
 
 Ψ
 
 
 S
 
 n
 
 
 
 
 (
 t
 )
 

 
 
 (
 
 Ψ
 (
 t
 )
 
 )
 
 
 n
 
 
 
 
 {\displaystyle \Psi _{S_{n}}(t)=\left(\Psi (t)\right)^{n}}
 .La covariance et la corrélation de deux variables aléatoires indépendantes sont nulles : 
 
 
 
 Corr
 
 (
 
 X
 
 1
 
 
 ,
 
 X
 
 2
 
 
 )
 

 Cov
 
 (
 
 X
 
 1
 
 
 ,
 
 X
 
 2
 
 
 )
 

 0
 
 
 {\displaystyle \operatorname {Corr} (X_{1},X_{2})=\operatorname {Cov} (X_{1},X_{2})=0}
 c'est-à-dire 
 
 
 
 
 E
 
 (
 
 X
 
 1
 
 
 
 X
 
 2
 
 
 )
 

 
 E
 
 (
 
 X
 
 1
 
 
 )
 
 E
 
 (
 
 X
 
 2
 
 
 )
 
 
 {\displaystyle \mathbb {E} (X_{1}X_{2})=\mathbb {E} (X_{1})\mathbb {E} (X_{2})}
 . Cependant une covariance nulle n'implique pas toujours l'indépendance des variables aléatoires. Dans le cas où les variables sont les coordonnées d'un vecteur gaussien, leur indépendance est équivalente au fait que leurs covariances deux à deux soient nulles .
Si 
 
 
 
 
 X
 
 1
 
 
 
 
 {\displaystyle X_{1}}
 et 
 
 
 
 
 X
 
 2
 
 
 
 
 {\displaystyle X_{2}}
 sont iid, alors : 
 
 
 
 Cov
 
 (
 
 X
 
 1
 
 
 +
 
 X
 
 2
 
 
 ,
 
 X
 
 1
 
 
 −
 
 X
 
 2
 
 
 )
 

 0
 
 
 {\displaystyle \operatorname {Cov} (X_{1}+X_{2},X_{1}-X_{2})=0}
 .

Exemples de sommes de variables iid.Si 
 
 
 
 
 X
 
 1
 
 
 ,
 
 X
 
 2
 
 
 ,
 …
 ,
 
 X
 
 n
 
 
 
 
 {\displaystyle X_{1},X_{2},\dots ,X_{n}}
 sont iid de loi de Bernoulli de même paramètre 
 
 
 
 p
 
 
 {\displaystyle p}
 , alors 
 
 
 
 
 X
 
 1
 
 
 +
 
 X
 
 2
 
 
 +
 ⋯
 +
 
 X
 
 n
 
 
 
 
 {\displaystyle X_{1}+X_{2}+\dots +X_{n}}
 est de loi binomiale : 
 
 
 
 b
 (
 n
 ,
 p
 )
 
 
 {\displaystyle b(n,p)}
 .
Si 
 
 
 
 
 X
 
 1
 
 
 ,
 
 X
 
 2
 
 
 ,
 …
 ,
 
 X
 
 n
 
 
 
 
 {\displaystyle X_{1},X_{2},\dots ,X_{n}}
 sont iid de loi géométrique de même paramètre 
 
 
 
 p
 
 
 {\displaystyle p}
 , alors 
 
 
 
 
 X
 
 1
 
 
 +
 
 X
 
 2
 
 
 +
 ⋯
 +
 
 X
 
 n
 
 
 
 
 {\displaystyle X_{1}+X_{2}+\dots +X_{n}}
 est de loi binomiale négative : 
 
 
 
 N
 e
 g
 B
 i
 n
 (
 n
 ,
 p
 )
 
 
 {\displaystyle NegBin(n,p)}
 .
Si 
 
 
 
 
 X
 
 1
 
 
 ,
 
 X
 
 2
 
 
 ,
 …
 ,
 
 X
 
 n
 
 
 
 
 {\displaystyle X_{1},X_{2},\dots ,X_{n}}
 sont iid de loi de Poisson de paramètres respectifs 
 
 
 
 
 λ
 
 1
 
 
 ,
 
 λ
 
 2
 
 
 ,
 …
 ,
 
 λ
 
 n
 
 
 
 
 {\displaystyle \lambda _{1},\lambda _{2},\dots ,\lambda _{n}}
 , alors 
 
 
 
 
 X
 
 1
 
 
 +
 
 X
 
 2
 
 
 +
 ⋯
 +
 
 X
 
 n
 
 
 
 
 {\displaystyle X_{1}+X_{2}+\dots +X_{n}}
 est de loi de Poisson : 
 
 
 
 
 
 P
 
 
 (
 
 λ
 
 1
 
 
 +
 
 λ
 
 2
 
 
 +
 ⋯
 +
 
 λ
 
 n
 
 
 )
 
 
 {\displaystyle {\mathcal {P}}(\lambda _{1}+\lambda _{2}+\dots +\lambda _{n})}
 .
Si 
 
 
 
 
 X
 
 1
 
 
 ,
 
 X
 
 2
 
 
 ,
 …
 ,
 
 X
 
 n
 
 
 
 
 {\displaystyle X_{1},X_{2},\dots ,X_{n}}
 sont iid de loi exponentielle de même paramètre 
 
 
 
 λ
 
 
 {\displaystyle \lambda }
 , alors 
 
 
 
 
 X
 
 1
 
 
 +
 
 X
 
 2
 
 
 +
 ⋯
 +
 
 X
 
 n
 
 
 
 
 {\displaystyle X_{1}+X_{2}+\dots +X_{n}}
 est de loi Gamma : 
 
 
 
 Γ
 (
 n
 ,
 λ
 )
 
 
 {\displaystyle \Gamma (n,\lambda )}
 .
Si 
 
 
 
 
 X
 
 1
 
 
 ,
 
 X
 
 2
 
 
 ,
 …
 ,
 
 X
 
 n
 
 
 
 
 {\displaystyle X_{1},X_{2},\dots ,X_{n}}
 sont iid de loi normale de paramètres respectifs 
 
 
 
 
 
 N
 
 
 (
 
 μ
 
 1
 
 
 ,
 
 σ
 
 1
 
 
 2
 
 
 )
 ,
 
 
 N
 
 
 (
 
 μ
 
 2
 
 
 ,
 
 σ
 
 2
 
 
 2
 
 
 )
 ,
 …
 ,
 
 
 N
 
 
 (
 
 μ
 
 n
 
 
 ,
 
 σ
 
 n
 
 
 2
 
 
 )
 
 
 {\displaystyle {\mathcal {N}}(\mu _{1},\sigma _{1}^{2}),{\mathcal {N}}(\mu _{2},\sigma _{2}^{2}),\dots ,{\mathcal {N}}(\mu _{n},\sigma _{n}^{2})}
 , alors 
 
 
 
 
 X
 
 1
 
 
 +
 
 X
 
 2
 
 
 +
 ⋯
 +
 
 X
 
 n
 
 
 
 
 {\displaystyle X_{1}+X_{2}+\dots +X_{n}}
 est de loi normale : 
 
 
 
 
 
 N
 
 
 (
 
 μ
 
 1
 
 
 +
 
 μ
 
 2
 
 
 +
 ⋯
 +
 
 μ
 
 n
 
 
 ,
 
 σ
 
 1
 
 
 2
 
 
 +
 
 σ
 
 2
 
 
 2
 
 
 +
 ⋯
 +
 
 σ
 
 n
 
 
 2
 
 
 )
 
 
 {\displaystyle {\mathcal {N}}(\mu _{1}+\mu _{2}+\dots +\mu _{n},\sigma _{1}^{2}+\sigma _{2}^{2}+\dots +\sigma _{n}^{2})}
 .
Si 
 
 
 
 
 X
 
 1
 
 
 ,
 
 X
 
 2
 
 
 ,
 …
 ,
 
 X
 
 n
 
 
 
 
 {\displaystyle X_{1},X_{2},\dots ,X_{n}}
 sont iid de loi normale de même paramètre 0 et 1, alors 
 
 
 
 
 X
 
 1
 
 
 2
 
 
 +
 
 X
 
 2
 
 
 2
 
 
 +
 ⋯
 +
 
 X
 
 n
 
 
 2
 
 
 
 
 {\displaystyle X_{1}^{2}+X_{2}^{2}+\dots +X_{n}^{2}}
 est de loi du χ² : 
 
 
 
 
 χ
 
 2
 
 
 (
 n
 )
 
 
 {\displaystyle \chi ^{2}(n)}
 .
Si 
 
 
 
 
 X
 
 1
 
 
 ,
 
 X
 
 2
 
 
 ,
 …
 ,
 
 X
 
 n
 
 
 
 
 {\displaystyle X_{1},X_{2},\dots ,X_{n}}
 sont iid de loi du χ² de paramètres respectifs 
 
 
 
 
 ν
 
 1
 
 
 ,
 
 ν
 
 2
 
 
 ,
 …
 
 ν
 
 n
 
 
 
 
 {\displaystyle \nu _{1},\nu _{2},\dots \nu _{n}}
 , alors 
 
 
 
 
 X
 
 1
 
 
 +
 
 X
 
 2
 
 
 +
 ⋯
 +
 
 X
 
 n
 
 
 
 
 {\displaystyle X_{1}+X_{2}+\dots +X_{n}}
 est de loi du χ² : 
 
 
 
 
 χ
 
 2
 
 
 (
 
 ν
 
 1
 
 
 +
 
 ν
 
 2
 
 
 +
 ⋯
 +
 
 ν
 
 n
 
 
 )
 
 
 {\displaystyle \chi ^{2}(\nu _{1}+\nu _{2}+\dots +\nu _{n})}
 .Une généralisation de la notion de variables iid est celle de variables échangeables (en) : les variables aléatoires réelles 
 
 
 
 
 X
 
 1
 
 
 ,
 
 X
 
 2
 
 
 ,
 …
 ,
 
 X
 
 n
 
 
 
 
 {\displaystyle X_{1},X_{2},\dots ,X_{n}}
 sont dites échangeables si la loi de probabilité du n-uplet 
 
 
 
 (
 
 X
 
 1
 
 
 ,
 
 X
 
 2
 
 
 ,
 …
 ,
 
 X
 
 n
 
 
 )
 
 
 {\displaystyle (X_{1},X_{2},\dots ,X_{n})}
 est la même que la loi de 
 
 
 
 (
 
 X
 
 σ
 (
 1
 )
 
 
 ,
 
 X
 
 σ
 (
 2
 )
 
 
 ,
 …
 ,
 
 X
 
 σ
 (
 n
 )
 
 
 )
 
 
 {\displaystyle (X_{\sigma (1)},X_{\sigma (2)},\dots ,X_{\sigma (n)})}
 pour toute permutation 
 
 
 
 σ
 
 
 {\displaystyle \sigma }
 . Autrement dit, la loi du n-uplet ne dépend pas de l'ordre des variables.
Si des variables réelles sont indépendantes et identiquement distribuées, alors elles sont échangeables. Il existe une réciproque partielle : des variables échangeables sont identiquement distribuées. Cependant, elles ne sont généralement pas indépendantes.
Le théorème central limite s'applique pour des variables iid, il existe des versions de ce théorème pour des variables échangeables.

# En statistique 

La statistique est une science qui étudie et interprète les données. Lorsque l'on possède une série de données, la question se pose de savoir si elles peuvent être modélisées par des variables iid. Autrement dit : sont-elles des valeurs différentes obtenues de manière indépendante d'un même phénomène aléatoire ? Les données peuvent être obtenues par mesures sur des individus ou objets qui doivent alors être choisis au hasard et avec remise; elles peuvent également provenir d'une simulation grâce à un générateur de nombres aléatoires : dans ce cas la graine du générateur doit être choisie aléatoirement et ne doit plus être changée après.
Il y a plusieurs méthodes pour tester l'indépendance des variables : la visualisation du corrélogramme (ou graphique d'autocorrélation), la visualisation du graphe de retard (Lag-plot) ou la réalisation de tests statistiques.

# Visualiser le graphique d’autocorrélation 

Dans le cas où la série est stationnaire, c'est-à-dire que les accroissements sont de covariance nulle, on peut s'intéresser à la fonction d'autocorrélation (ACF). Elle est définie comme la corrélation entre la première valeur et la 
 
 
 
 k
 
 
 {\displaystyle k}
 -ième valeur. Cette fonction permet d'évaluer si les données sont issues d'un modèle iid, elle est nulle si les données sont parfaitement iid. 
Il est à noter que plus il y a de valeurs, plus la fonction d'autocorrélation est petite. En fait les valeurs de la fonction d'autocorrélation décroissent en fonction de la taille 
 
 
 
 n
 
 
 {\displaystyle n}
 du nombre de données suivant le ratio 
 
 
 
 
 
 
 
 ±
 1
 ,
 96
 
 
 n
 
 
 
 
 
 
 {\displaystyle \textstyle {\frac {\pm 1,96}{\sqrt {n}}}}
 .

# Visualiser le graphe de retard 

Le graphique de retard (ou Lag-plot) 
 
 
 
 h
 
 
 {\displaystyle h}
 est un nuage de points qui a pour abscisses les valeurs 
 
 
 
 
 x
 
 1
 
 
 ,
 
 x
 
 2
 
 
 ,
 …
 ,
 
 x
 
 n
 
 
 
 
 {\displaystyle x_{1},x_{2},\dots ,x_{n}}
 de la série de données et en ordonnées ces valeurs décalées de 
 
 
 
 h
 
 
 {\displaystyle h}
 : 
 
 
 
 
 x
 
 1
 +
 h
 
 
 ,
 
 x
 
 2
 +
 h
 
 
 ,
 …
 ,
 
 x
 
 n
 +
 h
 
 
 
 
 {\displaystyle x_{1+h},x_{2+h},\dots ,x_{n+h}}
 en prenant l'indice modulo 
 
 
 
 n
 
 
 {\displaystyle n}
 , c'est-à-dire en reprenant au début de la liste lorsque l'on arrive à la fin. Si un des nuages fait apparaître une tendance d'orientation, la série n'est pas considérée iid.
Dans la galerie d'images ci-dessous, sont représentés 5 lag-plot pour n=500 valeurs iid de loi uniforme simulées avec scilab. En abscisse : les 500 valeurs 
 
 
 
 
 x
 
 1
 
 
 ,
 
 x
 
 2
 
 
 ,
 …
 ,
 
 x
 
 500
 
 
 
 
 {\displaystyle x_{1},x_{2},\dots ,x_{500}}
 ; en ordonnées : les valeurs décalées de h 
 
 
 
 
 x
 
 1
 +
 h
 
 
 ,
 
 x
 
 2
 +
 h
 
 
 ,
 …
 ,
 
 x
 
 500
 
 
 ,
 
 x
 
 1
 
 
 ,
 …
 ,
 
 x
 
 h
 
 
 
 
 {\displaystyle x_{1+h},x_{2+h},\dots ,x_{500},x_{1},\dots ,x_{h}}
 . On remarque que les 5 graphiques n'ont pas de tendance d'orientation, ce qui est le cas de variables iid.

Graphe de retard de valeurs iid
 
 
 
 
 
Dans la galerie d'images ci-dessous, sont représentés 5 lag-plot pour n=500 valeurs qui ne sont pas iid. Les valeurs 
 
 
 
 
 x
 
 1
 
 
 ,
 
 x
 
 2
 
 
 ,
 …
 ,
 
 x
 
 500
 
 
 
 
 {\displaystyle x_{1},x_{2},\dots ,x_{500}}
 sont construites comme le maximum de valeurs iid : 
 
 
 
 
 x
 
 2
 
 
 

 m
 a
 x
 (
 
 y
 
 1
 
 
 ,
 
 y
 
 2
 
 
 )
 
 
 {\displaystyle x_{2}=max(y_{1},y_{2})}
 et 
 
 
 
 
 x
 
 3
 
 
 

 m
 a
 x
 (
 
 y
 
 2
 
 
 ,
 
 y
 
 3
 
 
 )
 
 
 {\displaystyle x_{3}=max(y_{2},y_{3})}
 ne sont pas indépendantes car elles dépendent toutes deux de 
 
 
 
 
 y
 
 2
 
 
 
 
 {\displaystyle y_{2}}
 . On remarque une tendance du nuage de points vers le haut et la droite, notamment une concentration linéaire sur le premier graphique.

Graphe de retard de valeurs non iid
 
 
 
 
 

# Effectuer des tests statistiques 

Il est possible de réaliser des tests statistiques pour vérifier si les données sont associées à une loi de probabilité (tests d'adéquation) et d'autres pour vérifier si les données sont indépendantes (test d'indépendance). Il existe aussi des tests pour vérifier si des valeurs sont iid, comme le test du point tournant (en),.
Si on note 
 
 
 
 
 x
 
 1
 
 
 ,
 
 x
 
 2
 
 
 ,
 …
 ,
 
 x
 
 j
 −
 1
 
 
 ,
 
 x
 
 j
 
 
 ,
 
 x
 
 j
 +
 1
 
 
 ,
 …
 ,
 
 x
 
 n
 
 
 
 
 {\displaystyle x_{1},x_{2},\dots ,x_{j-1},x_{j},x_{j+1},\dots ,x_{n}}
 les valeurs obtenues, on dit que la suite de valeurs est monotone en 
 
 
 
 j
 
 
 {\displaystyle j}
 si 
 
 
 
 
 x
 
 j
 −
 1
 
 
 ≤
 
 x
 
 j
 
 
 ≤
 
 x
 
 j
 +
 1
 
 
 
 
 {\displaystyle x_{j-1}\leq x_{j}\leq x_{j+1}}
 ou 
 
 
 
 
 x
 
 j
 −
 1
 
 
 ≥
 
 x
 
 j
 
 
 ≥
 
 x
 
 j
 +
 1
 
 
 
 
 {\displaystyle x_{j-1}\geq x_{j}\geq x_{j+1}}
 . Dans le cas contraire, on dit que 
 
 
 
 j
 
 
 {\displaystyle j}
 est un point tournant. Intuitivement cela signifie que les valeurs ne sont pas ordonnées. Si les valeurs 
 
 
 
 
 x
 
 1
 
 
 ,
 
 x
 
 2
 
 
 ,
 …
 ,
 
 x
 
 n
 
 
 
 
 {\displaystyle x_{1},x_{2},\dots ,x_{n}}
 sont issues d'un modèle iid, alors le nombre de points tournants suit une loi normale lorsque 
 
 
 
 n
 
 
 {\displaystyle n}
 tend vers 
 
 
 
 ∞
 
 
 {\displaystyle \infty }
 . Il est alors possible de faire un test statistique : on teste 
 
 
 
 
 H
 
 0
 
 
 

 [
 
 x
 
 1
 
 
 ,
 
 x
 
 2
 
 
 ,
 …
 ,
 
 x
 
 n
 
 
 
 sont 
 
 i
 i
 d
 .
 ]
 
 
 {\displaystyle H_{0}=[x_{1},x_{2},\dots ,x_{n}{\text{ sont }}iid.]}
 contre 
 
 
 
 
 H
 
 1
 
 
 

 [
 
 x
 
 1
 
 
 ,
 
 x
 
 2
 
 
 ,
 …
 ,
 
 x
 
 n
 
 
 
 ne sont pas 
 
 i
 i
 d
 .
 ]
 
 
 {\displaystyle H_{1}=[x_{1},x_{2},\dots ,x_{n}{\text{ ne sont pas }}iid.]}
 .
Si 
 
 
 
 
 T
 
 n
 
 
 
 
 {\displaystyle T_{n}}
 est le nombre de points tournants pour 
 
 
 
 n
 
 
 {\displaystyle n}
 valeurs, alors 
 
 
 
 
 T
 
 n
 
 
 
 
 {\displaystyle T_{n}}
 suit la loi 
 
 
 
 
 
 
 N
 
 
 
 (
 
 
 
 
 2
 n
 −
 4
 
 3
 
 
 ,
 
 
 
 16
 n
 −
 29
 
 90
 
 
 
 )
 
 
 
 
 {\displaystyle \textstyle {\mathcal {N}}\left({\frac {2n-4}{3}},{\frac {16n-29}{90}}\right)}
 . Si 
 
 
 
 
 P
 
 (
 Z
 <
 
 t
 
 α
 
 
 )
 

 α
 
 
 {\displaystyle \mathbb {P} (Z<t_{\alpha })=\alpha }
 avec 
 
 
 
 Z
 
 
 {\displaystyle Z}
 de loi normale centrée réduite, alors la valeur critique est :

 
 
 
 
 v
 
 α
 
 
 

 
 
 
 2
 n
 −
 4
 
 3
 
 
 +
 
 t
 
 α
 
 
 
 
 
 
 16
 n
 −
 29
 
 90
 
 
 
 
 
 {\displaystyle v_{\alpha }={\frac {2n-4}{3}}+t_{\alpha }{\sqrt {\frac {16n-29}{90}}}}
 .C'est-à-dire, si le nombre de points tournants est plus petit que 
 
 
 
 
 v
 
 α
 
 
 
 
 {\displaystyle v_{\alpha }}
 , alors il est considéré comme trop petit et l'hypothèse iid est rejetée avec un risque 
 
 
 
 α
 
 
 {\displaystyle \alpha }
 de se tromper.
Par exemple, la série 
 
 
 
 1
 ,
 2
 ,
 3
 ,
 5
 ,
 4
 ,
 6
 ,
 7
 ,
 8
 
 
 {\displaystyle 1,2,3,5,4,6,7,8}
 contient 2 points tournants pour 
 
 
 
 n
 

 8
 
 
 {\displaystyle n=8}
 valeurs. Pour un risque de 
 
 
 
 α
 

 5
 %
 
 
 {\displaystyle \alpha =5\%}
 , la table de valeur de la loi normale donne 
 
 
 
 
 t
 
 α
 
 
 

 −
 1
 ,
 645
 
 
 {\displaystyle t_{\alpha }=-1,645}
 . Ainsi : 
 
 
 
 
 v
 
 α
 
 
 

 2
 ,
 27
 >
 T
 

 2
 
 
 {\displaystyle v_{\alpha }=2,27>T=2}
 . On rejette l'hypothèse et on conclut que les valeurs ne sont pas issues d'un modèle iid avec un risque de 5 % de se tromper. Cet exemple est réalisé avec une faible valeur de 
 
 
 
 n
 
 
 {\displaystyle n}
 , ce qui ne le rend pas très performant.

# Simulations 

Il est très utile de pouvoir simuler des valeurs issues de variables aléatoires identiquement distribuées. Lorsque l'on utilise une expérience aléatoire comme un lancer de dé, le résultat n'est en rien déterministe et on obtient alors des valeurs qui dépendent totalement du hasard. Cependant, il est compliqué d'automatiser des mécanismes physiques, on utilise alors des outils adaptés pour effectuer de grands nombres de calculs, comme l'ancienne méthode des tables de nombres aléatoires : la société RAND Corporation propose ainsi en 1955 une table d'un million de valeurs pseudo-aléatoires. Le développement de l'informatique au XXe siècle a permis la création de générateurs de valeurs aléatoires. Le but est de simuler une liste de valeurs iid le mieux possible. L'utilisation d'un algorithme déterministe, c'est-à-dire qui n'utilise pas d'expérience aléatoire, permet de simuler une liste de valeurs possédant presque les mêmes propriétés que des valeurs aléatoires. On parle dans ce cas de valeurs pseudo-aléatoires si elles sont, : 

Non prévisibles : c'est-à-dire que la suite de valeurs ne peut pas être compressée, au sens où il est impossible de la décrire en utilisant moins de place que celle qu’elle occupe ;
Équiréparties sur plusieurs entiers prédéfinis : c'est-à-dire que les fréquences d'apparition de chaque entier sont identiques.Il n'est cependant pas possible d'obtenir un hasard indépendant parfait, c'est-à-dire équitablement réparties et ne dépendant pas les unes des autres. Deux problèmes se posent : obtenir des valeurs qui suivent la loi de probabilité recherchée et qui sont indépendantes entre elles. Il existe plusieurs méthodes pour simuler des valeurs iid, certaines sont plus rapides mais donnent des résultats moins probants, c'est-à-dire que l'équirépartition et l'imprévisibilité ne sont pas parfaites, on peut parler de hasard faible. D'autres méthodes utilisant des phénomènes physiques permettent d'obtenir de meilleurs résultats, on parle de hasard fort.

La plupart des générateurs de nombres aléatoires utilisent la loi uniforme continue pour générer d'autres lois de probabilité. Par exemple, si on considère une variable 
 
 
 
 U
 
 
 {\displaystyle U}
 de loi uniforme, alors la variable 
 
 
 
 V
 
 
 {\displaystyle V}
 définie par : 
 
 
 
 V
 

 1
 
 
 {\displaystyle V=1}
 si 
 
 
 
 U
 <
 p
 
 
 {\displaystyle U<p}
 et 
 
 
 
 V
 

 0
 
 
 {\displaystyle V=0}
 si 
 
 
 
 U
 ≥
 p
 
 
 {\displaystyle U\geq p}
 , est une variable aléatoire de loi de Bernoulli. Une manière plus générale est construite sur le résultat suivant : 

si 
 
 
 
 F
 
 
 {\displaystyle F}
 est la fonction de répartition de la loi de probabilité recherchée et 
 
 
 
 U
 
 
 {\displaystyle U}
 est une variable aléatoire de loi uniforme sur 
 
 
 
 [
 0
 ,
 1
 ]
 
 
 {\displaystyle [0,1]}
 , alors 
 
 
 
 
 F
 
 −
 1
 
 
 (
 U
 )
 
 
 {\displaystyle F^{-1}(U)}
 est une variable aléatoire qui suit la loi de probabilité voulue,. Ici 
 
 
 
 
 F
 
 −
 1
 
 
 (
 u
 )
 

 sup
 {
 x
 ∈
 
 R
 
 ;
 F
 (
 x
 )
 ≤
 u
 }
 
 
 {\displaystyle F^{-1}(u)=\sup\{x\in \mathbb {R} ;F(x)\leq u\}}
 est la transformée inverse.

Il reste alors à simuler une suite de valeurs de loi uniforme continue sur 
 
 
 
 [
 0
 ,
 1
 ]
 
 
 {\displaystyle [0,1]}
 et indépendantes. Un des algorithmes classiques est basé sur les congruences de fonctions linéaires. Il permet de construire une suite de valeurs obtenues par itération :

 
 
 
 
 
 {
 
 
 
 
 X
 
 n
 +
 1
 
 
 

 a
 
 X
 
 n
 
 
 
 mod
 
 (
 m
 )
 
 
 
 
 
 X
 
 0
 
 
 
 fixé 
 
 
 
 
 
 
 
 
 
 {\displaystyle {\begin{cases}X_{n+1}=aX_{n}\;\operatorname {mod} (m)\\X_{0}{\text{ fixé }}\end{cases}}}
 où :

le module 
 
 
 
 m
 
 
 {\displaystyle m}
 réalise la congruence, c'est-à-dire que toutes les valeurs seront dans l'ensemble 
 
 
 
 {
 0
 ,
 1
 ,
 2
 ,
 …
 ,
 m
 −
 1
 }
 
 
 {\displaystyle \{0,1,2,\dots ,m-1\}}
 ,
le multiplicateur 
 
 
 
 a
 ∈
 {
 1
 ,
 2
 ,
 …
 ,
 m
 −
 1
 }
 
 
 {\displaystyle a\in \{1,2,\dots ,m-1\}}
 est la fonction linéaire,
la valeur initiale 
 
 
 
 
 X
 
 0
 
 
 
 
 {\displaystyle X_{0}}
 est appelée la graine ou le germe.Cet algorithme est chaotique dans le sens où un petit changement de la valeur initiale entraine des résultats différents. Sous de bonnes conditions sur 
 
 
 
 a
 
 
 {\displaystyle a}
 et 
 
 
 
 m
 
 
 {\displaystyle m}
 , les valeurs de la suite sont considérées comme des réalisations de variables iid. Par exemple, 
 
 
 
 a
 

 
 2
 
 7
 
 
 −
 1
 
 
 {\displaystyle a=2^{7}-1}
 et 
 
 
 
 m
 

 
 2
 
 31
 
 
 −
 1
 
 
 {\displaystyle m=2^{31}-1}
 donnent de bons résultats ; la valeur de la graine peut être choisie aléatoirement en utilisant l'horloge interne de l'ordinateur par exemple. Les valeurs pseudo-aléatoires obtenues prennent les valeurs 
 
 
 
 0
 ,
 1
 ,
 2
 ,
 …
 ,
 m
 −
 1
 
 
 {\displaystyle 0,1,2,\dots ,m-1}
 de manière équiprobable, ainsi en divisant par la grande valeur 
 
 
 
 m
 
 
 {\displaystyle m}
 , on obtient des valeurs considérées uniformes sur 
 
 
 
 [
 0
 ,
 1
 ]
 
 
 {\displaystyle [0,1]}
 .

# Applications et exemples 

# Exemple classique 

Un exemple de tirage iid est celui d'un jeu de pile ou face. Chaque lancer de la même pièce suit la même loi de Bernoulli de paramètre 
 
 
 
 p
 
 
 {\displaystyle p}
 et est indépendant de ceux qui l'ont précédé ou vont lui succéder.
Dans cet exemple simple, le tirage pourrait ne plus être iid. On peut utiliser deux pièces différentes dont l'une n'est pas équilibrée comme l'autre. Par exemple une première pièce avec la probabilité 
 
 
 
 
 p
 
 1
 
 
 
 
 {\displaystyle p_{1}}
 d'obtenir pile et l'autre avec probabilité 
 
 
 
 
 p
 
 2
 
 
 ≠
 
 p
 
 1
 
 
 
 
 {\displaystyle p_{2}\neq p_{1}}
 .

si on utilise alternativement les deux pièces biaisées, les résultats ne suivent pas la même loi de probabilité, ils restent néanmoins indépendants ;
si on lance systématiquement la pièce ayant le plus de chance de donner face après chaque tirage pile, les tirages ne sont plus indépendants car conditionnés par le résultat du tirage précédent. Les valeurs ne suivent pas non plus la même loi.

# Théorèmes limites 

En théorie des probabilités et en statistique, il existe des théorèmes limites : c'est-à-dire une convergence lorsque le nombre de variables tend vers l'infini. Parmi les théorèmes limites les plus régulièrement étudiés figurent la loi des grands nombres et le théorème central limite. Ces deux théorèmes s'appliquent dans le cas d'une répétition de 
 
 
 
 n
 
 
 {\displaystyle n}
 fois la même expérience, c'est-à-dire dans le cas de 
 
 
 
 n
 
 
 {\displaystyle n}
 variables indépendantes et identiquement distribuées. Voici leurs énoncés :

Loi des grands nombres : si 
 
 
 
 
 X
 
 1
 
 
 ,
 
 X
 
 2
 
 
 ,
 …
 ,
 
 X
 
 n
 
 
 
 
 {\displaystyle X_{1},X_{2},\dots ,X_{n}}
 sont des variables réelles indépendantes et identiquement distribuées d'espérance finie 
 
 
 
 μ
 
 
 {\displaystyle \mu }
 et de variance finie 
 
 
 
 
 σ
 
 2
 
 
 
 
 {\displaystyle \sigma ^{2}}
 , alors la variable 
 
 
 
 
 
 
 1
 n
 
 
 (
 
 X
 
 1
 
 
 +
 
 X
 
 2
 
 
 +
 ⋯
 +
 
 X
 
 n
 
 
 )
 
 
 
 {\displaystyle \textstyle {\frac {1}{n}}(X_{1}+X_{2}+\dots +X_{n})}
 converge en probabilité vers 
 
 
 
 μ
 
 
 {\displaystyle \mu }
 .
Théorème central limite : si 
 
 
 
 
 X
 
 1
 
 
 ,
 
 X
 
 2
 
 
 ,
 …
 ,
 
 X
 
 n
 
 
 
 
 {\displaystyle X_{1},X_{2},\dots ,X_{n}}
 sont des variables réelles indépendantes et identiquement distribuées d'espérance finie 
 
 
 
 μ
 
 
 {\displaystyle \mu }
 et de variance finie 
 
 
 
 
 σ
 
 2
 
 
 
 
 {\displaystyle \sigma ^{2}}
 , alors la variable 
 
 
 
 
 
 
 1
 
 σ
 
 
 n
 
 
 
 
 
 (
 
 X
 
 1
 
 
 +
 
 X
 
 2
 
 
 +
 ⋯
 +
 
 X
 
 n
 
 
 −
 n
 μ
 )
 
 
 
 {\displaystyle \textstyle {\frac {1}{\sigma {\sqrt {n}}}}(X_{1}+X_{2}+\dots +X_{n}-n\mu )}
 convergent en loi vers une variable aléatoire de loi normale centrée réduite.L'hypothèse iid de ces théorèmes est suffisante mais n'est pas nécessaire. Par exemple, la loi des grands nombres s'applique également lorsque la covariance des variables 
 
 
 
 (
 
 X
 
 i
 
 
 
 )
 
 i
 ≥
 1
 
 
 
 
 {\displaystyle (X_{i})_{i\geq 1}}
 converge vers 0 lorsque la distance entre les indices grandit : 
 
 
 
 
 lim
 
 j
 →
 +
 ∞
 
 
 R
 (
 j
 )
 

 0
 
 
 {\displaystyle \lim _{j\rightarrow +\infty }R(j)=0}
 avec 
 
 
 
 R
 (
 j
 −
 i
 )
 

 Cov
 
 (
 
 X
 
 i
 
 
 ,
 
 X
 
 j
 
 
 )
 
 
 {\displaystyle R(j-i)=\operatorname {Cov} (X_{i},X_{j})}
 .
Au XXe siècle, Benoît Mandelbrot désigne différents types de hasard : ceux dits « bénins » qui vérifient les hypothèses des théorèmes limites précédents et ceux plus « sauvages » ou « chaotiques » qui ne vérifient pas l'hypothèse iid. Autrement dit, ces deux théorèmes permettent en un certain sens de classifier des types de hasard.

# Processus et bruit blanc 

Un processus stochastique représente l'évolution en fonction du temps d'une variable aléatoire. Il est alors prenant de s'intéresser aux accroissements des processus.
Pour un processus stochastique 
 
 
 
 (
 
 X
 
 t
 
 
 
 )
 
 t
 ≥
 0
 
 
 
 
 {\displaystyle (X_{t})_{t\geq 0}}
 , si pour toutes variables 
 
 
 
 
 t
 
 1
 
 
 <
 
 t
 
 2
 
 
 <
 ⋯
 <
 
 t
 
 n
 
 
 
 
 {\displaystyle t_{1}<t_{2}<\dots <t_{n}}
 , les accroissements 
 
 
 
 
 X
 
 
 t
 
 2
 
 
 
 
 −
 
 X
 
 
 t
 
 1
 
 
 
 
 ,
 
 X
 
 
 t
 
 3
 
 
 
 
 −
 
 X
 
 
 t
 
 2
 
 
 
 
 ,
 …
 ,
 
 X
 
 
 t
 
 n
 
 
 
 
 −
 
 X
 
 
 t
 
 n
 −
 1
 
 
 
 
 
 
 {\displaystyle X_{t_{2}}-X_{t_{1}},X_{t_{3}}-X_{t_{2}},\dots ,X_{t_{n}}-X_{t_{n-1}}}
 sont indépendants, alors le processus est dit à accroissements indépendants. De plus, si la loi de probabilité des accroissements 
 
 
 
 
 X
 
 t
 
 
 −
 
 X
 
 t
 +
 h
 
 
 
 
 {\displaystyle X_{t}-X_{t+h}}
 , pour tout 
 
 
 
 h
 >
 0
 
 
 {\displaystyle h>0}
 ne dépend pas de 
 
 
 
 t
 
 
 {\displaystyle t}
 , alors le processus est dit stationnaire.
Il existe de nombreux cas de processus stationnaires à accroissements indépendants : la marche aléatoire dans le cas où la variable de temps 
 
 
 
 t
 
 
 {\displaystyle t}
 est un nombre entier, le processus de Wiener ou mouvement brownien dans le cas où 
 
 
 
 
 X
 
 t
 
 
 
 
 {\displaystyle X_{t}}
 suit une loi normale 
 
 
 
 
 
 N
 
 
 (
 0
 ,
 t
 )
 
 
 {\displaystyle {\mathcal {N}}(0,t)}
 , le processus de Poisson et plus généralement les processus de Markov, le processus de Lévy.
Un autre exemple de processus à accroissements indépendants et stationnaires est le bruit blanc. C'est un processus stochastique 
 
 
 
 (
 
 X
 
 t
 
 
 
 )
 
 t
 ≥
 0
 
 
 
 
 {\displaystyle (X_{t})_{t\geq 0}}
 tel que : pour tous 
 
 
 
 t
 
 
 {\displaystyle t}
 et 
 
 
 
 h
 
 
 {\displaystyle h}
 ,

 
 
 
 
 E
 
 (
 
 X
 
 t
 
 
 )
 

 m
 
 
 {\displaystyle \mathbb {E} (X_{t})=m}
 , l'espérance est constante ;

 
 
 
 Var
 
 (
 
 X
 
 t
 
 
 )
 

 
 σ
 
 2
 
 
 
 
 {\displaystyle \operatorname {Var} (X_{t})=\sigma ^{2}}
 , la variance est constante ;

 
 
 
 Cov
 
 (
 
 X
 
 t
 
 
 ,
 
 X
 
 t
 +
 h
 
 
 )
 

 0
 
 
 {\displaystyle \operatorname {Cov} (X_{t},X_{t+h})=0}
 , la covariance est nulle.On dit que c'est un processus iid car il est formé de variables indépendantes et identiquement distribuées. Si de plus la loi de 
 
 
 
 
 X
 
 t
 
 
 
 
 {\displaystyle X_{t}}
 est normale alors le bruit blanc est parfois dit « gaussien » et est noté nid (normal et identiquement distribué).

# Méthode de Monte-Carlo 

Les méthodes de Monte-Carlo permettent d'estimer numériquement des résultats grâce à l'usage de valeurs aléatoires ; elles servent à éviter les calculs exacts (elles ont l'avantage de réduire le temps de calculs ou encore d'approximer un résultat dont le calcul exact est impossible à réaliser). Plus précisément, la méthode de Monte-Carlo ordinaire utilise des variables indépendantes et identiquement distribuées 
 
 
 
 
 X
 
 1
 
 
 ,
 
 X
 
 2
 
 
 ,
 …
 ,
 
 X
 
 n
 
 
 
 
 {\displaystyle X_{1},X_{2},\dots ,X_{n}}
 . Grâce au théorème central limite, la variable aléatoire 
 
 
 
 
 
 
 
 
 μ
 ^
 
 
 
 
 n
 
 
 −
 
 
 1
 n
 
 
 (
 
 X
 
 1
 
 
 +
 
 X
 
 2
 
 
 +
 ⋯
 +
 
 X
 
 n
 
 
 )
 
 
 
 {\displaystyle \textstyle {\hat {\mu }}_{n}-{\frac {1}{n}}(X_{1}+X_{2}+\dots +X_{n})}
 converge en loi vers une variable aléatoire de loi normale. Grâce à la connaissance de la loi normale, on peut donc obtenir une valeur approchée de la valeur recherchée pour un grand nombre de variables aléatoires considérées. Ainsi la moyenne empirique 
 
 
 
 
 
 
 
 μ
 ^
 
 
 
 
 n
 
 
 
 
 {\displaystyle {\hat {\mu }}_{n}}
 est une valeur approchée de la vraie moyenne recherchée. Plus précisément, on peut obtenir un intervalle de confiance de la moyenne au seuil de confiance 95 % : 

 
 
 
 [
 
 
 
 
 μ
 ^
 
 
 
 
 n
 
 
 −
 1
 ,
 96
 
 
 
 
 
 
 σ
 ^
 
 
 
 
 n
 
 
 
 n
 
 
 
 
 ;
 
 
 
 
 
 μ
 ^
 
 
 
 
 n
 
 
 +
 1
 ,
 96
 
 
 
 
 
 
 σ
 ^
 
 
 
 
 n
 
 
 
 n
 
 
 
 ]
 
 
 {\displaystyle [{\hat {\mu }}_{n}-1,96{\frac {{\hat {\sigma }}_{n}}{\sqrt {n}}}\,;\,{\hat {\mu }}_{n}+1,96{\frac {{\hat {\sigma }}_{n}}{\sqrt {n}}}]}
 où 
 
 
 
 
 
 
 
 σ
 ^
 
 
 
 
 n
 
 
 

 
 
 1
 n
 
 
 
 ∑
 
 i
 

 1
 
 
 n
 
 
 (
 
 X
 
 i
 
 
 −
 
 
 
 
 μ
 ^
 
 
 
 
 n
 
 
 
 )
 
 2
 
 
 
 
 {\displaystyle {\hat {\sigma }}_{n}={\frac {1}{n}}\sum _{i=1}^{n}(X_{i}-{\hat {\mu }}_{n})^{2}}
 .Cette méthode sert également à réaliser d'autres types de calculs, tel que le calcul d'intégrale, ou permet également de donner une méthode pour simuler différentes variables d'une loi prédéfinie, en utilisant par exemple l'algorithme de Metropolis-Hastings ou l'échantillonnage de Gibbs.
La méthode de Monte-Carlo existe aussi dans le cas où les variables ne sont plus iid, on parle alors de méthode de Monte-Carlo par chaînes de Markov. La suite de variables aléatoires est alors une chaîne de Markov stationnaire. La perte de l'hypothèse iid complique les résultats.

# Notes et références 

Articles et autres sources

# Voir aussi 

# Bibliographie 

 : document utilisé comme source pour la rédaction de cet article.

Patrick Bogaert, Probabilités pour scientifiques et ingénieurs: Introduction au calcul des probabilités, De Boeck Supérieur, 2005, 402 p. (ISBN 2-8041-4794-0, lire en ligne)
Régis Bourbonnais et Michel Terraza, Analyse des séries temporelles, Dunod, 2010, 352 p. (ISBN 978-2-10-056111-7, lire en ligne)
(en) Steve Brooks, Andrew Gelman, Galin Jones et Xiao-Li Meng, Handbook of Markov Chain Monte Carlo, CRC Press, 2011, 619 p. (ISBN 978-1-4200-7942-5, lire en ligne)
Michel Henry, Autour de la modélisation en probabilités, Presses Univ. Franche-Comté, 2001, 258 p. (ISBN 2-84627-018-X, lire en ligne)
(en) Jean-Yves Le Boudec, Performance Evaluation of Computer and Communication Systems, EPFL, 2015, 359 p. (lire en ligne [PDF])
Michel Lejeune, Statistique: la théorie et ses applications, Springer Science & Business Media, 2004, 339 p. (ISBN 2-287-21241-8, lire en ligne)
Stephan Morgenthaler, Introduction à la statistique, Presse polytechniques, 2007, 387 p. (ISBN 978-1-4710-3427-5, lire en ligne)
(en) Thomas A. Severini, Elements of Distribution Theory, Cambridge University Press, 2005, 515 p. (ISBN 978-0-521-84472-7, lire en ligne)

# Articles connexes 

Loi de probabilité
Indépendance (probabilités)
Variable indépendante
 Portail des probabilités et de la statistique
